# Default values for triliovault.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

---
storage: ceph

labels:
  datamover_api:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  agent:
    datamover:
      node_selector_key: openstack-compute-node
      node_selector_value: enabled


release_group: null

images:
  tags:
    test: docker.io/xrally/xrally-openstack:2.0.0
    db_init: docker.io/openstackhelm/heat:ussuri-ubuntu_bionic
    triliovault_db_sync: docker.io/trilio/triliovault-helm:victoria-ubuntu_bionic
    db_drop: docker.io/openstackhelm/heat:ussuri-ubuntu_bionic
    rabbit_init: docker.io/rabbitmq:3.7-management
    ks_user: docker.io/openstackhelm/heat:ussuri-ubuntu_bionic
    ks_service: docker.io/openstackhelm/heat:ussuri-ubuntu_bionic
    ks_endpoints: docker.io/openstackhelm/heat:ussuri-ubuntu_bionic
    bootstrap: docker.io/openstackhelm/heat:ussuri-ubuntu_bionic
    dep_check: quay.io/airshipit/kubernetes-entrypoint:v1.0.0
    image_repo_sync: docker.io/docker:17.07.0
    trilio_datamover_api: docker.io/trilio/trilio-datamover-api-helm:victoria-ubuntu_bionic
    trilio_datamover: docker.io/trilio/trilio-datamover-helm:victoria-ubuntu_bionic
    trilio_horizon_plugin: docker.io/trilio/trilio-horizon-plugin-helm:victoria-ubuntu_bionic
    trilio_workloadmgr_api: docker.io/trilio/trilio-workloadmgr-api-helm:victoria-ubuntu_bionic
    trilio_workloadmgr_scheduler: docker.io/trilio/trilio-workloadmgr-scheduler-helm:victoria-ubuntu_bionic
    trilio_workloadmgr_cron: docker.io/trilio/trilio-workloadmgr-cron-helm:victoria-ubuntu_bionic
    trilio_workloadmgr_workloads: docker.io/trilio/trilio-workloadmgr-workloads-helm:victoria-ubuntu_bionic
  pull_policy: "IfNotPresent"
  local_registry:
    active: false
    exclude:
      - dep_check
      - image_repo_sync

jobs:
  volume_usage_audit:
    cron: "5 * * * *"
    starting_deadline: 600
    history:
      success: 3
      failed: 1

pod:
  security_context:
    volume_usage_audit:
      pod:
        runAsUser: 42424
      container:
        triliovault_volume_usage_audit:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
    datamover_api:
      pod:
        runAsUser: 42424
      container:
        ceph_coordination_volume_perms:
          runAsUser: 0
          readOnlyRootFilesystem: true
        datamover_api:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
    datamover:
      pod:
        runAsUser: 42426
      container:
        ceph_backup_keyring_placement:
          runAsUser: 0
          readOnlyRootFilesystem: true
        ceph_keyring_placement:
          runAsUser: 0
          readOnlyRootFilesystem: true
        ceph_backup_volume_perms:
          runAsUser: 0
          readOnlyRootFilesystem: true
        ceph_coordination_volume_perms:
          runAsUser: 0
          readOnlyRootFilesystem: true
        triliovault_backup:
          capabilities:
            add:
              - SYS_ADMIN
          readOnlyRootFilesystem: true
          runAsUser: 0
    clean:
      pod:
        runAsUser: 42424
      container:
        triliovault_volume_rbd_secret_clean:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
    create_internal_tenant:
      pod:
        runAsUser: 42424
      container:
        create_internal_tenant:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
  affinity:
    anti:
      type:
        default: preferredDuringSchedulingIgnoredDuringExecution
      topologyKey:
        default: kubernetes.io/hostname
      weight:
        default: 10
  useHostNetwork:
    volume: false
    backup: false
  mounts:
    triliovault_datamover_api:
      init_container: null
      datamover_api:
        volumeMounts:
        volumes:
    triliovault_datamover:
      init_container: null
      datamover:
        volumeMounts:
        volumes:
    triliovault_datamover_tests:
      init_container: null
      datamover_tests:
        volumeMounts:
        volumes:
    triliovault_db_sync:
      triliovault_db_sync:
        volumeMounts:
        volumes:
  replicas:
    datamover_api: 1
    datamover: 1
  lifecycle:
    upgrades:
      deployments:
        revision_history: 3
        pod_replacement_strategy: RollingUpdate
        rolling_update:
          max_unavailable: 1
          max_surge: 3
    disruption_budget:
      datamover_api:
        min_available: 0
    termination_grace_period:
      datamover_api:
        timeout: 30
  resources:
    enabled: false
    datamover_api:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    datamover:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    jobs:
      volume_usage_audit:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      bootstrap:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      rabbit_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_drop:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      clean:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      backup_storage_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      storage_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_endpoints:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_service:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_user:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      tests:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"

bootstrap:
  enabled: true
  ks_user: admin
  bootstrap_conf_backends: true
  volume_types:
    name:
      group:
      volume_backend_name:
  # Volume QoS if any. By default, None QoS is created.
  # Below values with a number at the end need to be replaced
  # with real names.
  # volume_qos:
  #   qos_name_1:
  #     consumer: front-end
  #     properties:
  #       key_1: value_1
  #       key_2: value_2
  #     associates:
  #       - volume_type_1
  #       - volume_type_2

network:
  datamover_datamover_api:
    ingress:
      public: true
      classes:
        namespace: "nginx"
        cluster: "nginx-cluster"
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    external_policy_local: false
    node_port:
      enabled: false
      port: 30877

ceph_client:
  # enable this when there is a need to create second ceph backed pointing
  # to external ceph cluster
  enable_external_ceph_backend: false
  # change this in case of first ceph backend name pointing to internal ceph cluster
  # is diffrent
  internal_ceph_backend: rbd1
  configmap: ceph-etc
  user_secret_name: pvc-ceph-client-key
  external_ceph:
    # Only when enable_external_ceph_backend is true and rbd_user is NOT null
    # secret for external ceph keyring will be created.
    rbd_user: null
    rbd_user_keyring: null
    conf:
      global: null
      osd: null
conf:
  triliovault_sudoers: |
    # This sudoers file supports rootwrap for both Kolla and LOCI Images.
    Defaults !requiretty
    Defaults secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/var/lib/openstack/bin:/var/lib/kolla/venv/bin"
    triliovault ALL = (root) NOPASSWD: /var/lib/kolla/venv/bin/triliovault-rootwrap /etc/triliovault/rootwrap.conf *, /var/lib/openstack/bin/triliovault-rootwrap /etc/triliovault/rootwrap.conf *
  datamover-api:
    DEFAULT:
      dmapi_workers = {{ dmapi_workers }}
      transport_url = {{ rpc_transport_url }}
      dmapi_link_prefix = http://{{ api_interface_address }}:{{ triliovault_datamover_api_port }}
      dmapi_enabled_ssl_apis =
      dmapi_listen_port = {{ triliovault_datamover_api_port }}
      dmapi_enabled_apis = dmapi
      bindir = /usr/bin
      instance_name_template = instance-%08x
      dmapi_listen = {{ api_interface_address }}
      my_ip = {{ api_interface_address }}
      rootwrap_config = /etc/dmapi/rootwrap.conf
      debug = False
      log_file = /var/log/kolla/triliovault-datamover-api/dmapi.log
      log_dir = /var/log/kolla/triliovault-datamover-api
    wsgi:
      ssl_cert_file = 
      ssl_key_file =
      api_paste_config = /etc/dmapi/api-paste.ini
    database:
      connection = mysql+pymysql://{{ triliovault_database_user }}:{{ triliovault_database_password }}@{{ triliovault_database_address }}/{{ triliovault_database_name }}
    keystone_authtoken:
      www_authenticate_uri = {{ keystone_internal_url }}
      auth_url = {{ keystone_admin_url }}
      auth_type = password
      project_name = service
      username = {{ triliovault_keystone_user }}
      password = {{ triliovault_keystone_password }}
      cafile = {{ openstack_cacert }}
      memcache_security_strategy = ENCRYPT
      memcache_secret_key = {{ memcache_secret_key }}
      memcached_servers = {% for host in groups['memcached'] %}{{ 'api' | kolla_address(host) | put_address_in_context('memcache') }}:{{ memcached_port }}{% if not loop.last %},{% endif %}{% endfor %}
      signing_dir = /var/cache/dmapi
      project_domain_name = {{ default_project_domain_name }}
      user_domain_name = {{ default_user_domain_name }}
      insecure = True
    oslo_messaging_notifications:
      transport_url = {{ notify_transport_url }}
      {% if triliovault_enabled_notification_topics %}
      driver = messagingv2
      topics = {{ triliovault_enabled_notification_topics | map(attribute='name') | join(',') }}
      {% else %}
      driver = noop
      {% endif %}
    oslo_middleware:
      enable_proxy_headers_parsing = True
  datamover:
    DEFAULT:
      {% if triliovault_backup_target == "nfs" %}
      vault_storage_type = nfs
      {% if multi_ip_nfs_enabled == "yes" %}
      vault_storage_nfs_export = "{{ triliovault_nfs_map[inventory_hostname] }}"
      {% else %}
      vault_storage_nfs_export = {{ triliovault_nfs_shares }}
      {% endif %}
      vault_storage_nfs_options = {{ triliovault_nfs_options }}
      {% elif triliovault_backup_target == "amazon_s3" %}
      vault_storage_type = s3
      vault_storage_nfs_export = TrilioVault
      vault_s3_auth_version = DEFAULT
      vault_s3_access_key_id = {{ triliovault_s3_access_key }}
      vault_s3_secret_access_key = {{ triliovault_s3_secret_key }}
      vault_s3_region_name = {{ triliovault_s3_region_name }}
      vault_s3_bucket = {{ triliovault_s3_bucket_name }}
      vault_s3_signature_version = {{ triliovault_s3_version }}
      vault_s3_auth_version = {{ triliovault_s3_auth_version }}
      vault_s3_ssl_cert =
      {% elif triliovault_backup_target == "other_s3_compatible" %}
      vault_storage_type = s3
      vault_storage_nfs_export = TrilioVault
      vault_s3_auth_version = DEFAULT
      vault_s3_access_key_id = {{ triliovault_s3_access_key }}
      vault_s3_secret_access_key = {{ triliovault_s3_secret_key }}
      vault_s3_region_name = {{ triliovault_s3_region_name }}
      vault_s3_bucket = {{ triliovault_s3_bucket_name }}
      vault_s3_endpoint_url = {{ triliovault_s3_endpoint_url }}
      vault_s3_signature_version = {{ triliovault_s3_version }}
      vault_s3_auth_version = {{ triliovault_s3_auth_version }}
      vault_s3_ssl = {{ triliovault_s3_ssl_enabled }}
      vault_s3_ssl_cert = /etc/tvault-contego/{{ triliovault_s3_ssl_cert_file_name }}
      {% endif %}
      vault_data_directory_old = /var/triliovault
      vault_data_directory = {{ triliovault_parent_data_directory }}/triliovault-mounts
      log_file = /var/log/kolla/triliovault-datamover/tvault-contego.log
      debug = False
      verbose = True
      max_uploads_pending = 3
      max_commit_pending = 3
      dmapi_transport_url = {{ rpc_transport_url }}
    dmapi_database:
      connection = mysql+pymysql://{{ triliovault_database_user }}:{{ triliovault_database_password }}@{{ triliovault_database_address }}/{{ triliovault_database_name }}
    {% if cinder_backend_ceph == "yes" %}
    libvirt:
      images_rbd_ceph_conf = /etc/ceph/ceph.conf
      rbd_user = {{ ceph_cinder_user }}
    ceph:
      keyring_ext = .{{ ceph_cinder_user }}.keyring
    {% endif %}
    contego_sys_admin:
      helper_command = sudo /usr/bin/privsep-helper
    {% if triliovault_backup_target == "other_s3_compatible" or triliovault_backup_target == "amazon_s3" %}
    s3fuse_sys_admin:
      helper_command = sudo /usr/bin/privsep-helper
    {% endif %}
    conductor:
      use_local = True
    oslo_messaging_rabbit:
      ssl = false
    cinder:
      http_retries = {{ cinder_http_retries }}

  logging:
    loggers:
      keys:
        - root
        - triliovault
    handlers:
      keys:
        - stdout
        - stderr
        - "null"
    formatters:
      keys:
        - context
        - default
    logger_root:
      level: WARNING
      handlers: 'null'
    logger_triliovault:
      level: INFO
      handlers:
        - stdout
      qualname: triliovault
    logger_amqp:
      level: WARNING
      handlers: stderr
      qualname: amqp
    logger_amqplib:
      level: WARNING
      handlers: stderr
      qualname: amqplib
    logger_eventletwsgi:
      level: WARNING
      handlers: stderr
      qualname: eventlet.wsgi.server
    logger_sqlalchemy:
      level: WARNING
      handlers: stderr
      qualname: sqlalchemy
    logger_boto:
      level: WARNING
      handlers: stderr
      qualname: boto
    handler_null:
      class: logging.NullHandler
      formatter: default
      args: ()
    handler_stdout:
      class: StreamHandler
      args: (sys.stdout,)
      formatter: context
    handler_stderr:
      class: StreamHandler
      args: (sys.stderr,)
      formatter: context
    formatter_context:
      class: oslo_log.formatters.ContextFormatter
      datefmt: "%Y-%m-%d %H:%M:%S"
    formatter_default:
      format: "%(message)s"
      datefmt: "%Y-%m-%d %H:%M:%S"
  rabbitmq:
    # NOTE(rk760n): adding rmq policy to mirror messages from notification queues and set expiration time for the ones
    policies:
      - vhost: "triliovault"
        name: "ha_ttl_triliovault"
        definition:
          # mirror messges to other nodes in rmq cluster
          ha-mode: "all"
          ha-sync-mode: "automatic"
          # 70s
          message-ttl: 70000
        priority: 0
        apply-to: all
        pattern: '^(?!(amq\.|reply_)).*'

  backends:
    # Those options will be written to backends.conf as-is.
    rbd1:
      volume_driver: triliovault.volume.drivers.rbd.RBDDriver
      volume_backend_name: rbd1
      rbd_pool: triliovault.volumes
      rbd_ceph_conf: "/etc/ceph/ceph.conf"
      rbd_flatten_volume_from_snapshot: false
      report_discard_supported: true
      rbd_max_clone_depth: 5
      rbd_store_chunk_size: 4
      rados_connect_timeout: -1
      rbd_user: triliovault
      rbd_secret_uuid: 457eb676-33da-42ec-9a8c-9293d545c337
      image_volume_cache_enabled: True
      image_volume_cache_max_size_gb: 200
      image_volume_cache_max_count: 50
  rally_tests:
    run_tempest: false
    clean_up: |
      VOLUMES=$(openstack volume list -f value | grep -e "^s_rally_" | awk '{ print $1 }')
      if [ -n "$VOLUMES" ]; then
        echo $VOLUMES | xargs openstack volume delete
      fi
    tests:
      triliovaultVolumes.create_and_delete_volume:
        - args:
            size: 1
          runner:
            concurrency: 1
            times: 1
            type: constant
          sla:
            failure_rate:
              max: 0
        - args:
            size:
              max: 5
              min: 1
          runner:
            concurrency: 1
            times: 1
            type: constant
          sla:
            failure_rate:
              max: 0
  resource_filters:
    volume:
      - name
      - status
      - metadata
      - bootable
      - migration_status
      - availability_zone
      - group_id
    backup:
      - name
      - status
      - volume_id
    snapshot:
      - name
      - status
      - volume_id
      - metadata
      - availability_zone
    group: []
    group_snapshot:
      - status
      - group_id
    attachment:
      - volume_id
      - status
      - instance_id
      - attach_status
    message:
      - resource_uuid
      - resource_type
      - event_id
      - request_id
      - message_level
    pool:
      - name
      - volume_type
    volume_type: []
  enable_iscsi: false
backup:
  external_ceph_rbd:
    enabled: false
    admin_keyring: null
    conf:
      global: null
      osd: null
  posix:
    volume:
      class_name: general
      size: 10Gi

dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs:
          - triliovault-image-repo-sync
        services:
          - endpoint: node
            service: local_image_registry
  static:
    api:
      jobs:
        - triliovault-db-sync
        - triliovault-ks-user
        - triliovault-ks-endpoints
        - triliovault-rabbit-init
        - triliovault-storage-init
      services:
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: identity
    backup:
      jobs:
        - triliovault-db-sync
        - triliovault-ks-user
        - triliovault-ks-endpoints
        - triliovault-rabbit-init
        - triliovault-storage-init
        - triliovault-backup-storage-init
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volumev3
    backup_storage_init:
      jobs: null
    bootstrap:
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volumev3
      pod:
        - requireSameNode: false
          labels:
            application: triliovault
            component: volume
    clean:
      jobs: null
    db_drop:
      services:
        - endpoint: internal
          service: oslo_db
    db_init:
      services:
        - endpoint: internal
          service: oslo_db
    db_sync:
      jobs:
        - triliovault-db-init
      services:
        - endpoint: internal
          service: oslo_db
    ks_endpoints:
      jobs:
        - triliovault-ks-service
      services:
        - endpoint: internal
          service: identity
    ks_service:
      services:
        - endpoint: internal
          service: identity
    ks_user:
      services:
        - endpoint: internal
          service: identity
    rabbit_init:
      services:
        - service: oslo_messaging
          endpoint: internal
    scheduler:
      jobs:
        - triliovault-db-sync
        - triliovault-ks-user
        - triliovault-ks-endpoints
        - triliovault-rabbit-init
        - triliovault-storage-init
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volumev3
    storage_init:
      jobs: null
    tests:
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volumev3
    volume:
      jobs:
        - triliovault-db-sync
        - triliovault-ks-user
        - triliovault-ks-endpoints
        - triliovault-rabbit-init
        - triliovault-storage-init
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volumev3
    volume_usage_audit:
      jobs:
        - triliovault-db-sync
        - triliovault-ks-user
        - triliovault-ks-endpoints
        - triliovault-rabbit-init
        - triliovault-storage-init
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volumev3
    image_repo_sync:
      services:
        - endpoint: internal
          service: local_image_registry
    create_internal_tenant:
      services:
        - endpoint: internal
          service: identity

# Names of secrets used by bootstrap and environmental checks
secrets:
  identity:
    admin: triliovault-keystone-admin
    triliovault: triliovault-keystone-user
    test: triliovault-keystone-test
  oslo_db:
    admin: triliovault-db-admin
    triliovault: triliovault-db-user
  rbd:
    backup: triliovault-backup-rbd-keyring
    volume: triliovault-volume-rbd-keyring
    volume_external: triliovault-volume-external-rbd-keyring
  oslo_messaging:
    admin: triliovault-rabbitmq-admin
    triliovault: triliovault-rabbitmq-user
  tls:
    datamover:
      datamover_api:
        public: triliovault-tls-public
        internal: triliovault-tls-api
# We use a different layout of the endpoints here to account for versioning
# this swaps the service name and type, and should be rolled out to other
# services.
endpoints:
  cluster_domain_suffix: cluster.local
  local_image_registry:
    name: docker-registry
    namespace: docker-registry
    hosts:
      default: localhost
      internal: docker-registry
      node: localhost
    host_fqdn_override:
      default: null
    port:
      registry:
        node: 5000
  identity:
    name: keystone
    auth:
      admin:
        region_name: RegionOne
        username: admin
        password: password
        project_name: admin
        user_domain_name: default
        project_domain_name: default
      triliovault:
        role: admin
        region_name: RegionOne
        username: dmapi
        password: password
        project_name: service
        user_domain_name: service
        project_domain_name: service
      test:
        role: admin
        region_name: RegionOne
        username: triliovault-test
        password: password
        project_name: test
        user_domain_name: service
        project_domain_name: service
    hosts:
      default: keystone
      internal: keystone-api
    host_fqdn_override:
      default: null
    path:
      default: /v3
    scheme:
      default: http
    port:
      api:
        default: 80
        internal: 5000
  image:
    name: glance
    hosts:
      default: glance-api
      public: glance
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme:
      default: http
    port:
      api:
        default: 9292
        public: 80
  volumev3:
    name: triliovault
    hosts:
      default: triliovault-api
      public: triliovault
    host_fqdn_override:
      default: null
      # NOTE(portdirect): this chart supports TLS for fqdn over-ridden public
      # endpoints using the following format:
      # public:
      #   host: null
      #   tls:
      #     crt: null
      #     key: null
    path:
      default: '/v3/%(tenant_id)s'
    scheme:
      default: 'http'
    port:
      api:
        default: 8776
        public: 80
  oslo_db:
    auth:
      admin:
        username: root
        password: password
        secret:
          tls:
            internal: mariadb-tls-direct
      triliovault:
        username: dmapi
        password: password
    hosts:
      default: mariadb
    host_fqdn_override:
      default: null
    path: /dmapi
    scheme: mysql+pymysql
    port:
      mysql:
        default: 3306
  oslo_messaging:
    auth:
      admin:
        username: rabbitmq
        password: password
        secret:
          tls:
            internal: rabbitmq-tls-direct
      triliovault:
        username: dmapi
        password: password
    statefulset:
      replicas: 2
      name: rabbitmq-rabbitmq
    hosts:
      default: rabbitmq
    host_fqdn_override:
      default: null
    path: /dmapi
    scheme: rabbit
    port:
      amqp:
        default: 5672
      http:
        default: 15672
  oslo_cache:
    auth:
      # NOTE(portdirect): this is used to define the value for keystone
      # authtoken cache encryption key, if not set it will be populated
      # automatically with a random value, but to take advantage of
      # this feature all services should be set to use the same key,
      # and memcache service.
      memcache_secret_key: null
    hosts:
      default: memcached
    host_fqdn_override:
      default: null
    port:
      memcache:
        default: 11211
  fluentd:
    namespace: null
    name: fluentd
    hosts:
      default: fluentd-logging
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme: 'http'
    port:
      service:
        default: 24224
      metrics:
        default: 24220
  kube_dns:
    namespace: kube-system
    name: kubernetes-dns
    hosts:
      default: kube-dns
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme: http
    port:
      dns:
        default: 53
        protocol: UDP
  ingress:
    namespace: null
    name: ingress
    hosts:
      default: ingress
    port:
      ingress:
        default: 80

network_policy:
  triliovault:
    ingress:
      - {}
    egress:
      - {}

# NOTE(helm_hook): helm_hook might break for helm2 binary.
# set helm3_hook: false when using the helm2 binary.
helm3_hook: true

manifests:
  certificates: false
  configmap_bin: true
  configmap_etc: true
  deployment_datamover_api: true
  deployment_datamover: true
  ingress_api: true
  job_backup_storage_init: true
  job_bootstrap: true
  job_clean: true
  job_create_internal_tenant: true
  job_db_init: true
  job_image_repo_sync: true
  job_rabbit_init: true
  job_db_sync: true
  job_db_drop: false
  job_ks_endpoints: true
  job_ks_service: true
  job_ks_user: true
  job_storage_init: true
  pdb_api: true
  pod_rally_test: true
  pvc_backup: true
  network_policy: false
  secret_db: true
  secret_ingress_tls: true
  secret_keystone: true
  secret_rabbitmq: true
  service_api: true
  service_ingress_api: true
...
